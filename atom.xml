<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Neverland</title>
  
  <subtitle>你是我不愿醒来的梦啊&lt;br&gt;真是柔情一场</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ir1d.cf/"/>
  <updated>2018-08-11T16:51:38.645Z</updated>
  <id>https://ir1d.cf/</id>
  
  <author>
    <name>Ir1d</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OI Wiki 征稿与招募启事</title>
    <link href="https://ir1d.cf/2018/08/12/%E5%BE%81%E7%A8%BF%E4%B8%8E%E6%8B%9B%E5%8B%9F%E5%90%AF%E4%BA%8B/"/>
    <id>https://ir1d.cf/2018/08/12/征稿与招募启事/</id>
    <published>2018-08-11T16:49:17.000Z</published>
    <updated>2018-08-11T16:51:38.645Z</updated>
    
    <content type="html"><![CDATA[<p>（本文得到了很多朋友的指点，十分感谢！）</p><p>文章于 11 日下午发于 <a href="https://zhuanlan.zhihu.com/p/41747466" target="_blank" rel="noopener">知乎</a>，以及 LOJ、 UOJ、 Judge Duck 的讨论区。</p><h1 id="OI-Wiki-征稿与招募启事"><a href="#OI-Wiki-征稿与招募启事" class="headerlink" title="OI Wiki 征稿与招募启事"></a>OI Wiki 征稿与招募启事</h1><p>咦？ <strong>OI Wiki</strong> 是个啥？其实是某大型游戏线上攻略啦，围观地址：<a href="https://oi-wiki.cf" target="_blank" rel="noopener">Getting Started - <strong>OI Wiki</strong></a></p><hr><p>亲爱的 <strong>OIer</strong> 同学们：</p><p><strong>OI Wiki</strong> 致力于成为一个免费开放且持续更新的知识整合站点，大家可以在这里学习到关于 <strong>OI</strong> 竞赛有趣又实用的知识。我们为大家准备了 <strong>OI</strong> 竞赛中的基础知识、常见题型、解题思路以及常用工具等内容，帮助大家更快速深入地学习 <strong>OI</strong> 竞赛。</p><p>目前，<strong>OI Wiki</strong> 正在完善各大方向的基础知识，以便于初学者更好地学习。<br>当然，<strong>OI Wiki</strong> 基于 <strong>OI</strong>，却不会局限于 <strong>OI</strong>。<strong>OI Wiki</strong> 也会努力完善以下内容：</p><ul><li><p>应用在 ACM-ICPC 竞赛中的进阶知识</p></li><li><p>算法竞赛中的优质题目</p></li></ul><p>与此同时， <strong>OI Wiki</strong> 源于社区，提倡知识自由，在未来也绝不会商业化，将始终保持独立自由的性质。</p><p>在过去的一个月中，<strong>OI Wiki</strong> 见证了 199 次更新，十余位热心同学参与了条目的编写与完善，项目也得到了很多朋友的大力支持。目前，我们已经完成了原计划中 28 % 的内容，工作仍然还处于初期阶段，在此诚挚向诸君征稿：</p><p>我们欢迎各种形式的 <strong>OI</strong> 相关的内容，不论是知识点、题目、赛制介绍，还是可能会使用到的工具软件，只要你觉得可以对别人有帮助，我们都热诚欢迎。</p><p>联系方式：<br>Telegram 群组： <a href="https://t.me/OIwiki" target="_blank" rel="noopener">@OIwiki</a> ；QQ 群：<a href="https://jq.qq.com/?_wv=1027&amp;k=5EfkM6K" target="_blank" rel="noopener">588793226</a>。<br>欢迎各位加入～</p><hr><h3 id="F-A-Q"><a href="#F-A-Q" class="headerlink" title="F.A.Q."></a><strong>F.A.Q.</strong></h3><p>Q：你们是为什么想要做这个 Wiki 的呢？</p><p>A：不知道你在学 <strong>OI</strong> 的时候，面对庞大的知识体系，有没有感到过迷茫无助的时候？<strong>OI Wiki</strong> 想要做的事情可能类似于“让更多竞赛资源不充裕的同学能方便地接触到训练资源”。当然这么表述也不完全，做 Wiki 的动机可能也很纯粹，只是简单地想要对 <strong>OI</strong> 的发展做出一点点微小的贡献吧。XD</p><p>Q：我很感兴趣，怎么参与呢？</p><p>A：<strong>OI Wiki</strong> 现在托管在 <a href="https://github.com/24OI/OI-wiki" target="_blank" rel="noopener">Github</a> 上，你可以直接访问这个 <a href="https://github.com/24OI/OI-wiki" target="_blank" rel="noopener">repo</a> 来查看最新进展。参与的途径包括在 <a href="https://github.com/24OI/OI-wiki" target="_blank" rel="noopener">Github</a> 上面开 Issue、Pull Request，或者在交流群中分享你的想法、直接向管理员投稿。目前，我们使用的框架是 <a href="https://mkdocs.readthedocs.io" target="_blank" rel="noopener">mkdocs</a>，支持 Markdown 格式（当然也支持插入数学公式）。</p><p>Q：可是我比较弱…… 不知道我能做点什么？</p><p>A：一切源于热爱。你可以协助其他人审核修改稿件，帮助我们宣传 <strong>OI Wiki</strong> ，为社区营造良好学习交流氛围！</p><p>Q：现在主要是谁在做这件事啊？感觉这是个大坑，真的能做好吗？</p><p>A：现在主要是一些退役老年选手在做这件事，靠的都是信仰。这种“用爱发电”的事情，困难重重，我们也不知道能不能做好。但是如果这件事能够推动 <strong>OI</strong> 的发展，也是蛮有意义的呢。我们希望在这里打的这个广告，可以帮助这个团队遇见更多志同道合的人。</p><p>Q：听说过 nocow 吧，<strong>OI Wiki</strong> 怎么保证我们添加的内容不会像 nocow 那样突然间就不见了呢？</p><p>A：我们把内容托管在 <a href="https://github.com/24OI/OI-wiki" target="_blank" rel="noopener">Github</a> 上面，即使我们的服务器翻车了，内容也不会丢失。另外，我们也会定期备份大家的心血，即使有一天 <a href="https://github.com/24OI/OI-wiki" target="_blank" rel="noopener">Github</a> 倒闭了（？），我们的内容也不会丢失。</p><p>Q：<strong>OI Wiki</strong> 好像现在大部分内容都是空的啊？</p><p>A：是的，目前进度只完成了 28%，还远远称不上是一个合格的 Wiki。在过去的一个月里，主要添加的内容也比较基础。所以在这里进行征稿和招募，希望可以遇到有同样想法的朋友，我们一起把 <strong>OI Wiki</strong> 完善起来。</p><hr><p>感谢你看到了最后，我们现在急需的，就是你的帮助。</p><p><strong>OI Wiki</strong> Team</p><p>2018.8</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;（本文得到了很多朋友的指点，十分感谢！）&lt;/p&gt;
&lt;p&gt;文章于 11 日下午发于 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/41747466&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎&lt;/a&gt;，以及 LOJ、 UO
      
    
    </summary>
    
      <category term="笔记" scheme="https://ir1d.cf/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="OIWiki" scheme="https://ir1d.cf/tags/OIWiki/"/>
    
  </entry>
  
  <entry>
    <title>derain2018部分小结</title>
    <link href="https://ir1d.cf/2018/08/12/derain2018%E9%83%A8%E5%88%86%E5%B0%8F%E7%BB%93/"/>
    <id>https://ir1d.cf/2018/08/12/derain2018部分小结/</id>
    <published>2018-08-11T16:42:43.000Z</published>
    <updated>2018-08-11T16:43:43.810Z</updated>
    
    <content type="html"><![CDATA[<p>直接粘过来好了，和大家分享一下</p><h2 id="xUnit"><a href="#xUnit" class="headerlink" title="xUnit"></a>xUnit</h2><ul><li>Spatial Activation instead of perpixel activation —&gt; learn</li><li>use fewer layers to match the performance (also fewer param)</li><li>use less training example</li><li>一个位置的信息通过 ReLU 的时候要么乘上0 要么乘1。 考虑改成乘 $e^{-d_k^2}$</li><li>xUnit 的结构既有 nonlineariry(ReLU)，又有spatial processing (depth-wise convolution)</li><li>可视化结果发现很多 xnet activation和1（白色）接近，说明更多的channel 参与了 denoising</li><li>它把denoising 的 network 直接用来去雨，得到很不错的结果</li></ul><h2 id="DID-MDN"><a href="#DID-MDN" class="headerlink" title="DID-MDN"></a>DID-MDN</h2><ul><li>一个网络可能不能很好处理不同密集程度的雨线</li><li>文中把去雨过程分成两部分，先对雨线密集程度进行分类，然后再进行去雨</li><li>分类的过程是用和后面去雨那部分一样的网络结构，相当于是两个 branch，后一个branch另外加上了雨线密集程度的信息。</li><li>为什么不直接用 vgg fine-tune 呢？因为 vgg tend to pay more attention to localize the discriminative objects，雨线相对较小，难以在 high-level feature 中 很好地 localize.</li><li>training detail</li><li>classfier 的 训练是two stage，刚开始单独训练 resudual feature extraction network，来估计 residual part，然后再用 estimation 结果当做输入带训练 classification sub-network。最后再 joint optimize</li><li>multi scale 是考虑雨线的大小不一样，用不同的 receptive field 来捕捉这方面信息。</li><li>表格信息（NLEDN 里面好像没有这篇里面自己效果这么好）</li></ul><h2 id="Removing-rain-based-on-a-cycle-generative-adversarial-network"><a href="#Removing-rain-based-on-a-cycle-generative-adversarial-network" class="headerlink" title="Removing rain based on a cycle generative adversarial network"></a>Removing rain based on a cycle generative adversarial network</h2><p>CycleGAN-based</p><ul><li>Industrial Electronics and Applications (ICIEA)</li><li>深研院的</li><li>Loss functions</li><li>Cycle loss: strengthen the generator ability to distinguish between rain line and background</li><li>Color loss: to solve the problem of color shift in deraining images.</li><li>Texturue loss (transform into grayscale)</li><li>Dark channel loss</li><li>TV Loss : obtain a smooth result</li></ul><h2 id="DualCNN"><a href="#DualCNN" class="headerlink" title="DualCNN"></a>DualCNN</h2><ul><li>DualCNN。将structure与details分别训练</li><li>loss 是 l2 loss</li><li>detail + structure</li><li>ignore detail_loss in derain</li><li>Our goal is to learn the filtered image which does not contain rich details. —&gt; $\gamma = 0$</li><li>用在 derain 的时候，structure 部分是用的 clean image</li></ul><h2 id="Residual-Guide-Network"><a href="#Residual-Guide-Network" class="headerlink" title="Residual-Guide Network"></a>Residual-Guide Network</h2><ul><li>把 Basline block 简单串联起来效果并不好，需要利用中间信息来引导后面的 block 提取新的信息。</li><li>每一个大 block 里面是不断循环的（相当于是用同样的卷积层） （kernel reuse）</li><li>后面的每一个小块都加上最开头的那个小块（propagate all information forward from output of the first layer within each block）</li><li>最后面用 1x1 卷积 来 merge 所有的中间结果得到最终结果</li><li>ResGuideNet_5 是有 5 个 大block 且 没有进行 merge的</li></ul><h2 id="Semi-supervised-CNN-for-Single-Image-Rain-Removal"><a href="#Semi-supervised-CNN-for-Single-Image-Rain-Removal" class="headerlink" title="Semi-supervised CNN for Single Image Rain Removal"></a>Semi-supervised CNN for Single Image Rain Removal</h2><ul><li>the short-of-training-sample and bias-to-supervised-sample issues can be evidently alleviated</li><li>use the network to  formulate the residual between an input and its expected clear image (as a mixture of gaussian distribution)</li><li>objective function is the combination of supervised data loss (least square loss) and unsupervised data loss</li><li>optimized by least square residuals on supervised samples</li><li><p>&amp; patch-wised MoG (P-MoG) loss on unsupervised ones</p></li><li><p>网络的输出是 negative residual (opposite of the rain layer)</p></li><li>非监督部分：最小化 negative 雨线分布的 log-likelihood</li></ul><h2 id="NLEDN"><a href="#NLEDN" class="headerlink" title="NLEDN"></a>NLEDN</h2><ul><li>non-locally enhanced encoder-decoder network</li><li>fully exploit hierarchical features &amp; capture the long-distance dependencies and structural information</li><li>multi-scale non-local enhancement (原图分成8x8块)</li><li>dense block 和 resguide 一样</li></ul><h2 id="Joint-Bi-Layer-Optimize"><a href="#Joint-Bi-Layer-Optimize" class="headerlink" title="Joint Bi-Layer Optimize"></a>Joint Bi-Layer Optimize</h2><ul><li>locate rain-dominated regions &amp; estimate dominant direction</li><li><p>extract rain patches from regions to model the rain pattern</p></li><li><p>保真项</p></li><li>三个 prior</li><li>Sparsity prior: CSR: centralized sparse representation</li><li>Rain direction prior: 假设不和雨线方向垂直的 gradients 不太可能是属于雨线</li><li>Rain layer prior：在B上 regularize 可能会 smooth out 一些 背景信息，这个项的作用是 用 weighted laplacian term</li><li><p>优化方法</p><ul><li>先更新B<ul><li>交替地更新 $\alpha$ 和 B</li></ul></li><li>后更新R<h2 id="Simulated-Unsupervised-S-U-learning"><a href="#Simulated-Unsupervised-S-U-learning" class="headerlink" title="Simulated+Unsupervised (S+U) learning"></a>Simulated+Unsupervised (S+U) learning</h2></li></ul></li><li><p>uses unlabeled real data to refine the synthetic images</p></li><li>refiner network</li><li>l_real指的是refine之后的合成图片(x_i’)和真实图片Y之间的loss。l_reg是原始合成图片x_i和被refine之后的合成图片的x_i’之间的loss。lambda是一个高参</li><li>Refiner的目标就是尽可能的糊弄判别器D，让判别器没有办法区分一个图片是real还是合成的。判别器D的目标正好相反，是尽可能的能够区分出来</li><li>self-regularization loss 就是让refine之后的图片像素点和原始的图片的像素点之间的差不要太大</li><li>用refined的历史图片作为一个buffer而不单单是当前的mini-batch来更新分类器</li><li>在每一轮分类器的训练中，我们先从当前的batch中采样b/2张图片，然后从大小为B的buffer中采样b/2张图片，合在一起来更新判别器的参数。然后这一轮之后，用新生成的b/2张图片来替换掉B中的b/2张图片</li><li>instead of global discriminator, out discriminator classifies all local image patches separately. 好处是输入更小，感受野更小，而且会对每张原图得到更多让判别器学习的指标</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;直接粘过来好了，和大家分享一下&lt;/p&gt;
&lt;h2 id=&quot;xUnit&quot;&gt;&lt;a href=&quot;#xUnit&quot; class=&quot;headerlink&quot; title=&quot;xUnit&quot;&gt;&lt;/a&gt;xUnit&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Spatial Activation instead of
      
    
    </summary>
    
      <category term="笔记" scheme="https://ir1d.cf/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GAN" scheme="https://ir1d.cf/tags/GAN/"/>
    
      <category term="derain" scheme="https://ir1d.cf/tags/derain/"/>
    
  </entry>
  
  <entry>
    <title>给mkdocs用上gitment</title>
    <link href="https://ir1d.cf/2018/08/12/%E7%BB%99mkdocs%E7%94%A8%E4%B8%8Agitment/"/>
    <id>https://ir1d.cf/2018/08/12/给mkdocs用上gitment/</id>
    <published>2018-08-11T16:35:51.000Z</published>
    <updated>2018-08-11T16:47:15.492Z</updated>
    
    <content type="html"><![CDATA[<p>我用的是 mkdocs-material 这个主题，作者代码、文档写得都很清楚，在 Github 上面也很活跃，反馈也很快。</p><p>换一套好用的评论系统，困难是 disqus 和 livere 访问速度都不太行，livere 更过分的是还要去调用一个 Google Analytics 的 js = =</p><p>决定试一下 gitment，发现用在 wiki 里面可以很好地避免标题过长，超过 issue 限制的问题（相对于个人博客乱写的标题而言）。</p><p>具体方法是改那个 <a href="https://github.com/24OI/OI-wiki/blob/master/static/disqus.html" target="_blank" rel="noopener">disqus.html</a></p><p>然后部署的时候 <a href="https://github.com/24OI/OI-wiki/blob/master/build.sh#L21：" target="_blank" rel="noopener">https://github.com/24OI/OI-wiki/blob/master/build.sh#L21：</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp ./static/disqus.html mkdocs-material/material/partials/integrations/disqus.html</span><br></pre></td></tr></table></figure><p>改的时候不要忘记留着 <a href="https://github.com/24OI/OI-wiki/blob/master/static/disqus.html#L7：" target="_blank" rel="noopener">https://github.com/24OI/OI-wiki/blob/master/static/disqus.html#L7：</a></p><p><code>&lt;h2 id=&quot;__comments&quot;&gt; foo &lt;/h2&gt;</code>，这个 id 是用来在右侧的目录里面加一个到 “评论区” 的跳转。</p><p>上面这顿操作实际上就是替换了 disqus 加载 js 的过程，由于偷懒，在设置（mkdocs.yml）里面还是要标记成正在使用 disqus。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我用的是 mkdocs-material 这个主题，作者代码、文档写得都很清楚，在 Github 上面也很活跃，反馈也很快。&lt;/p&gt;
&lt;p&gt;换一套好用的评论系统，困难是 disqus 和 livere 访问速度都不太行，livere 更过分的是还要去调用一个 Google 
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="mkdocs" scheme="https://ir1d.cf/tags/mkdocs/"/>
    
      <category term="gitment" scheme="https://ir1d.cf/tags/gitment/"/>
    
  </entry>
  
  <entry>
    <title>pytorch小结</title>
    <link href="https://ir1d.cf/2018/08/03/pytorch%E5%B0%8F%E7%BB%93/"/>
    <id>https://ir1d.cf/2018/08/03/pytorch小结/</id>
    <published>2018-08-03T11:52:26.000Z</published>
    <updated>2018-08-03T16:19:41.615Z</updated>
    
    <content type="html"><![CDATA[<p>官方 tutorial 里面下下来的 cifar-10-python 大小是 341M。= = 请做好心理准备</p><p>backward 的时候是把 gradient 加上去，所以每次操作一个新的 mini-batch 的时候，都要手动先 <code>.zero_grad()</code></p><p>另外好像现在不用每个东西后面跟一堆 <code>.cuda()</code>，好像直接 <code>net.to(device)</code> 就好了？唯一需要注意的就是输入输出也要在相同的 device 上面？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;官方 tutorial 里面下下来的 cifar-10-python 大小是 341M。= = 请做好心理准备&lt;/p&gt;
&lt;p&gt;backward 的时候是把 gradient 加上去，所以每次操作一个新的 mini-batch 的时候，都要手动先 &lt;code&gt;.zero_gr
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="python" scheme="https://ir1d.cf/tags/python/"/>
    
      <category term="pytorch" scheme="https://ir1d.cf/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>conda重置environment状态</title>
    <link href="https://ir1d.cf/2018/08/02/conda%E9%87%8D%E7%BD%AEenvironment%E7%8A%B6%E6%80%81/"/>
    <id>https://ir1d.cf/2018/08/02/conda重置environment状态/</id>
    <published>2018-08-02T08:56:27.000Z</published>
    <updated>2018-08-04T01:23:03.416Z</updated>
    
    <content type="html"><![CDATA[<p>看到一个 comment 是说 conda 应该 base 留空，然后各种包开新的 environment 实验。</p><p>可是我已经装了一堆东西，又不想重装 conda = =</p><p>搜了下发现还是有这个 feature 的。</p><p><a href="https://stackoverflow.com/questions/41914139/how-to-reset-anaconda-root-environment" target="_blank" rel="noopener">https://stackoverflow.com/questions/41914139/how-to-reset-anaconda-root-environment</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda list --revisions</span><br><span class="line">conda install --rev 0</span><br></pre></td></tr></table></figure><p>0 就是要回退到哪个版本</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;看到一个 comment 是说 conda 应该 base 留空，然后各种包开新的 environment 实验。&lt;/p&gt;
&lt;p&gt;可是我已经装了一堆东西，又不想重装 conda = =&lt;/p&gt;
&lt;p&gt;搜了下发现还是有这个 feature 的。&lt;/p&gt;
&lt;p&gt;&lt;a href=
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="python" scheme="https://ir1d.cf/tags/python/"/>
    
      <category term="conda" scheme="https://ir1d.cf/tags/conda/"/>
    
  </entry>
  
  <entry>
    <title>pix2pix踩坑</title>
    <link href="https://ir1d.cf/2018/08/01/pix2pix%E8%B8%A9%E5%9D%91/"/>
    <id>https://ir1d.cf/2018/08/01/pix2pix踩坑/</id>
    <published>2018-07-31T16:30:25.000Z</published>
    <updated>2018-07-31T16:36:29.325Z</updated>
    
    <content type="html"><![CDATA[<p>找了个 pytorch 实现的 pix2pix 踩了不少坑== </p><p>用的这个人的：<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p><p>注意 doc 里面说可以怎么怎么样，然而代码里写的是 pix2pix 必须得用正方形的图= =</p><p>然后趁机掌握了 cv2 resize 的各种姿势= =</p><p>看 options 里面有很多选项，其实直接用 scripts 文件夹里现成的就好了</p><p>要注意的是训练和测试的方向= =</p><p>看好自己要 AtoB 还是 BtoA</p><p>（以及有个 qa.md 和 tips.md）根本没有链接，藏在代码里头</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># train</span><br><span class="line">python train.py --dataroot ./AB / --name rain --model pix2pix --which_direction AtoB --gpu_ids 0,1 --fineSize 480 --resize_or_crop crop</span><br><span class="line"></span><br><span class="line"># test</span><br><span class="line">python test.py --dataroot ./AB --name deraindrop_pix2pix --model pix2pix --which_model_netG unet_256 --which_direction AtoB --dataset_mode aligned --norm batch --resize_or_crop none</span><br></pre></td></tr></table></figure><p>上面好像并不对</p><p>还有个参数，是控制测试的时候用多少张的，默认是 50 = =，要手动改成自己的才行。</p><hr><p>mark 一下尝试跑通了 attentive-GAN</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python test.py --mode demo --input_dir ./100/input/ --output_dir ./100/output/</span><br></pre></td></tr></table></figure><p>hmmmm test.py 我自己重写了，大概就是把 demo 和 test 合到一起= =，既存结果又测 psnr。</p><p>要占用 7G+ 的显存= =</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;找了个 pytorch 实现的 pix2pix 踩了不少坑== &lt;/p&gt;
&lt;p&gt;用的这个人的：&lt;a href=&quot;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&quot; target=&quot;_blank&quot; rel=&quot;noo
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="python" scheme="https://ir1d.cf/tags/python/"/>
    
      <category term="pix2pix" scheme="https://ir1d.cf/tags/pix2pix/"/>
    
  </entry>
  
  <entry>
    <title>python-opencv</title>
    <link href="https://ir1d.cf/2018/07/30/python-opencv/"/>
    <id>https://ir1d.cf/2018/07/30/python-opencv/</id>
    <published>2018-07-30T02:36:08.000Z</published>
    <updated>2018-07-30T02:45:43.111Z</updated>
    
    <content type="html"><![CDATA[<h2 id="打开多张图片后内存不足的问题"><a href="#打开多张图片后内存不足的问题" class="headerlink" title="打开多张图片后内存不足的问题"></a>打开多张图片后内存不足的问题</h2><p><a href="https://stackoverflow.com/questions/19414289/python-and-memory-consumption-when-opening-tiff-images" target="_blank" rel="noopener">https://stackoverflow.com/questions/19414289/python-and-memory-consumption-when-opening-tiff-images</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure><h2 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h2><p><code>conda clean --all</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;打开多张图片后内存不足的问题&quot;&gt;&lt;a href=&quot;#打开多张图片后内存不足的问题&quot; class=&quot;headerlink&quot; title=&quot;打开多张图片后内存不足的问题&quot;&gt;&lt;/a&gt;打开多张图片后内存不足的问题&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://stack
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="opencv" scheme="https://ir1d.cf/tags/opencv/"/>
    
      <category term="python" scheme="https://ir1d.cf/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>ss在局域网共享</title>
    <link href="https://ir1d.cf/2018/07/30/ss%E5%9C%A8%E5%B1%80%E5%9F%9F%E7%BD%91%E5%85%B1%E4%BA%AB/"/>
    <id>https://ir1d.cf/2018/07/30/ss在局域网共享/</id>
    <published>2018-07-30T00:48:19.000Z</published>
    <updated>2018-07-30T00:49:44.491Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/shadowsocks/shadowsocks-libev/blob/master/doc/ss-local.asciidoc" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-libev/blob/master/doc/ss-local.asciidoc</a></p><p>看这里有一个 <code>-b &lt;local_address&gt;</code></p><p>在 <code>ss-local</code> 的命令后面加上 <code>-b 0.0.0.0</code> 就好了= =</p><p>如果是用配置文件，写法是 <a href="https://github.com/shadowsocks/shadowsocks-libev/issues/1441#issuecomment-293134272" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-libev/issues/1441#issuecomment-293134272</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/shadowsocks/shadowsocks-libev/blob/master/doc/ss-local.asciidoc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://githu
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="shadowsocks" scheme="https://ir1d.cf/tags/shadowsocks/"/>
    
  </entry>
  
  <entry>
    <title>为网页上选中部分设置高亮</title>
    <link href="https://ir1d.cf/2018/07/29/%E4%B8%BA%E7%BD%91%E9%A1%B5%E4%B8%8A%E9%80%89%E4%B8%AD%E9%83%A8%E5%88%86%E8%AE%BE%E7%BD%AE%E9%AB%98%E4%BA%AE/"/>
    <id>https://ir1d.cf/2018/07/29/为网页上选中部分设置高亮/</id>
    <published>2018-07-28T16:07:34.000Z</published>
    <updated>2018-07-29T08:11:05.096Z</updated>
    
    <content type="html"><![CDATA[<p>喜欢 vijos 可以在题面上做标注的那个功能好久了……</p><p>今天决定动手写一下</p><p>搜了好久搜不出来什么方法，发现可以获取 selection 的 range 然后操作一下，懵逼。不知道如果选中的是两个标签内的咋整。</p><p>后来学习了一下 vijos 的 <a href="https://github.com/vijos/vj4/blob/master/vj4/ui/components/marker/Marker.js" target="_blank" rel="noopener">源码</a>，发现可以用 <code>HiliteColor</code> 这个东西。搜了下，就发现了一些方法。</p><p><a href="https://stackoverflow.com/questions/2756931/highlight-the-text-of-the-dom-range-element" target="_blank" rel="noopener">https://stackoverflow.com/questions/2756931/highlight-the-text-of-the-dom-range-element</a></p><p>至于怎么做悬浮工具栏还没有想好，目前用的 <a href="https://stackoverflow.com/a/48422455/4597306" target="_blank" rel="noopener">这个</a>，自己的配色也很丑，功能和 vijos 比差远了= =</p><p>不过写出来了还是好赞啊 XD</p><hr><p>遇到了一个问题，因为全局监听的是 <code>mouseup</code>，然后触发 <code>popup-tag</code> 的 <code>click</code> 就会冲突。</p><p>搜到应该用一个 <code>event.stopPropagation();</code></p><p>还有一个坑是发现莫名奇妙 <code>click</code> 的时候 selection 变成空的了。</p><p>第一个想法是 select 的时候就存下当前的 range，结果发现竟然每 select 一次都会存下来，然后 click 的时候会把所有存下来的 range 都触发= =</p><p>解决办法是把 <code>popup-tag</code> 的 <code>click</code> 换成了 <code>mousedown</code></p><p>哎…… vijos 是真的厉害</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;喜欢 vijos 可以在题面上做标注的那个功能好久了……&lt;/p&gt;
&lt;p&gt;今天决定动手写一下&lt;/p&gt;
&lt;p&gt;搜了好久搜不出来什么方法，发现可以获取 selection 的 range 然后操作一下，懵逼。不知道如果选中的是两个标签内的咋整。&lt;/p&gt;
&lt;p&gt;后来学习了一下 vi
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="js" scheme="https://ir1d.cf/tags/js/"/>
    
      <category term="前端" scheme="https://ir1d.cf/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>运维小结</title>
    <link href="https://ir1d.cf/2018/07/28/%E8%BF%90%E7%BB%B4%E5%B0%8F%E7%BB%93/"/>
    <id>https://ir1d.cf/2018/07/28/运维小结/</id>
    <published>2018-07-28T15:51:16.000Z</published>
    <updated>2018-07-29T03:02:15.683Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><p>莫名其妙挂掉= =</p><p>讲道理之前要么内存不够要么磁盘满了，这回都没事，结果 <code>service mongod restart</code> 就是没反应，而且 log 里面也没写，非常懵逼。</p><h3 id="查看磁盘空间"><a href="#查看磁盘空间" class="headerlink" title="查看磁盘空间"></a>查看磁盘空间</h3><p><code>df -hl</code> 看各个磁盘</p><p><code>du -sh [目录名]</code> 返回该目录的大小</p><p>然后发现 pm2 的 log 占了 4G。原来是 mongod 挂掉期间执行了一次 schedule，然后不停循环= =</p><p>作死用 vim 打开，结果生成了 1G 的 .swp = =</p><p>在自动补全都不好使的情况下努力给删掉了 = =</p><p>发现还是跑不起来</p><h3 id="查看-log"><a href="#查看-log" class="headerlink" title="查看 log"></a>查看 log</h3><p><code>systemctl status mongod.service</code></p><p><code>journalctl -xe</code></p><p>我开始的时候是 <code>exit-code 100</code> 找到 <code>/var/lib/mongod</code> 下的 <code>mongod.lock</code> 删掉就好了= =</p><p>之前看到一堆人说要删掉，结果自己没发现自己文件夹有这东西</p><p>后来变成 1 了，然后就好了。</p><h3 id="MongoDB-数据备份"><a href="#MongoDB-数据备份" class="headerlink" title="MongoDB 数据备份"></a>MongoDB 数据备份</h3><p><a href="http://www.runoob.com/mongodb/mongodb-mongodump-mongorestore.html" target="_blank" rel="noopener">http://www.runoob.com/mongodb/mongodb-mongodump-mongorestore.html</a></p><p>用 <code>&gt;mongodump -h dbhost -d dbname -o dbdirectory</code></p><h3 id="MongoDB-相关"><a href="#MongoDB-相关" class="headerlink" title="MongoDB 相关"></a>MongoDB 相关</h3><p>log 位置 <code>/var/log/mongod/mongod.log</code></p><p>删了 log 之后要 touch 一个，以及要改 log 的 chown</p><p>数据库位置 <code>/var/lib/mongod/</code> 注意这里面要 chown 给 mongodb 这个用户</p><p><strong>不要用 root 开</strong></p><h2 id="Judge-Duck"><a href="#Judge-Duck" class="headerlink" title="Judge Duck"></a>Judge Duck</h2><p>= =</p><p>掌握了 Django 技能，尝试了多种前端框架</p><p>论这活真难= =</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MongoDB&quot;&gt;&lt;a href=&quot;#MongoDB&quot; class=&quot;headerlink&quot; title=&quot;MongoDB&quot;&gt;&lt;/a&gt;MongoDB&lt;/h2&gt;&lt;p&gt;莫名其妙挂掉= =&lt;/p&gt;
&lt;p&gt;讲道理之前要么内存不够要么磁盘满了，这回都没事，结果 &lt;code&gt;
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="运维" scheme="https://ir1d.cf/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="MongoDB" scheme="https://ir1d.cf/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.17 - 18</title>
    <link href="https://ir1d.cf/2018/07/17/dailynotes-8/"/>
    <id>https://ir1d.cf/2018/07/17/dailynotes-8/</id>
    <published>2018-07-17T01:01:42.000Z</published>
    <updated>2018-07-19T02:38:56.883Z</updated>
    
    <content type="html"><![CDATA[<p>感觉周末好像不在状态啊，一直在浪，昨天换了设备之后迁移一晚上，然后又开始浪= =</p><ul><li>Improved Techniques for Learning to Dehaze and Beyond: A Collective Study</li></ul><a id="more"></a><h2 id="Improved-Techniques-for-Learning-to-Dehaze-and-Beyond-A-Collective-Study"><a href="#Improved-Techniques-for-Learning-to-Dehaze-and-Beyond-A-Collective-Study" class="headerlink" title="Improved Techniques for Learning to Dehaze and Beyond: A Collective Study"></a>Improved Techniques for Learning to Dehaze and Beyond: A Collective Study</h2><p>仔细研究研究（</p><p>本文介绍了在新提出的 RESIDE 数据集上面进行的两方面的工作，一个是尝试在单张图片上去雾（low-level），另一个是尝试在有雾的图片上进行 high-level 的视觉理解（比如 object detection）。对于第一个工作，文中尝试了多种 loss function，并得到结论是 perception-driven loss 可以显著地提升去雾性能。对于第二个工作，作者得到很多解决方案，包括在检测及去雾过程中使用更先进的模型，以及 Domain Adaptive 的 object detection 方法。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>由于多种因素（air pollution, dust, mist, fume 等等），室外拍摄的照片往往是有复杂的、非线性的、和数据相关的噪声（统称为 haze），这对很多 high-level 的视觉任务（比如 object detection 和 recognition）产生一定的挑战。因此 dehazing 是一个研究很广泛的领域，早期的工作经常需要额外的关于场景 depth 的信息，或者是通过比对多张照片获得的信息。后来，很多方法提出对自然图片学习先验并做一些统计上的分析。近来，dehazing 算法应用了神经网络计数，效果很好。文中举了 AOD-Net 做例子，那篇工作中训练了一个 end-to-end 的去雾 + 检测（用的是 Faster R-CNN）的模型。</p><p>文中主要是两个任务，一是想要提升单张图片的去雾效果（作为 image-restoration 的问题），另一个是希望提升有雾情况下的 object detection 性能。（第一次看到有内容是加粗的……） <strong>作者提倡对于提升物体检测性能给予更多的关注，因为是 final goal</strong>。和 low-resolution，noise，blur 不同的是，haze 对人的视觉影响不大（甚至有时还会获得额外的美学效果），但是 haze 在室外环境中不受约束，可以对影响视觉系统的功能。拿自动tian驾驶举例，hazy 和 foggy 的天气会对车载摄像头产生影响，比如造成 reflection 和 glare。</p><h3 id="去雾方法"><a href="#去雾方法" class="headerlink" title="去雾方法"></a>去雾方法</h3><script type="math/tex; mode=display">I(x) = J(x)t(x) + A(1 - t(x))</script><p>其中 $I(x)$ 是观察到的有雾的图， $J(x)$ 是需要恢复出来的干净背景。参数 $A$ 代表全局环境光，$t(x) = e^{-\beta d(x)}$ 是 transmission 矩阵。$\beta$ 是散射系数， $d(x)$ 代表物体和镜头的距离。传统的单张图去雾方法一般是学习自然图片的先验，比如 dark channel prior， color-attenuation prior，和 non-local color cluster prior，再对、做一些统计学上的分析来复原 transmission matrix t(x)。进来，CNN 被应用到去雾模型，比如 multiscale CNN (MSCNN)，DehazeNet，AODNet。</p><h3 id="把去雾看做图像修复"><a href="#把去雾看做图像修复" class="headerlink" title="把去雾看做图像修复"></a>把去雾看做图像修复</h3><p>在这篇工作中，我们尝试找到和人类感知更 match 的 loss function 来训练去雾神经网络。我们用 AOD-Net （原网络用 MSE loss）来做骨架并用如下 loss function 替换：</p><ul><li>$l_1$ Loss:</li></ul><script type="math/tex; mode=display">L^{l_1}(P) = \frac{1}{N}\sum_{p \in P}|x(p) - y(p)|</script><p>这里 N 是图像块 P 中的像素个数，p 是用来表示一个像素位置，x(p) 和 y(p) 用来分别表示生成的图像和 ground truth 图像的像素值</p><ul><li><p>SSIM Loss:</p><script type="math/tex; mode=display">\begin{aligned}SSIM(p) & = \frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1} \frac{2\sigma_{xy}+C_2}{\sigma_x^2+\sigma_y^2+C_2} \\& = l(p)cs(p)\end{aligned}</script><p>其中平均值和标准差是通过一个标准差是 $\sigma_G$ 的高斯滤波器，这样就可以得到对于 SSIM 的 loss function 了。</p><script type="math/tex; mode=display">L^{SSIM}(p) = \frac{1}{N}\sum_{p \in P}1 - SSIM(p)</script></li><li><p>MS-SSIM Loss:<br>上面的式子中 $\sigma_G$ 的选择会影响到 SSIM 的训练效果。在这篇工作中，我们选用了 multi-scale SSIM ，即提前选好 M 个不同的 $\sigma_G$ 并进行融合：</p><script type="math/tex; mode=display">L^{MS-SSIM}(P) = l_M^{\alpha}(p) \prod_{j = 1}^Mcs_j^{\beta_j}(p)</script></li><li><p>MS-SSIM + $l_2$ Loss:<br>使用一个把 MS-SSIM loss 和 $l_2$ loss 加权合并起来的结果：</p><script type="math/tex; mode=display">L^{MS-SSIM-l_2} = \alpha \cdot L^{MS-SSIM} + (1 - \alpha) \cdot G_{\sigma_G^M} \cdot L^{l_2}</script><p>这里给 $l<em>2$ loss 逐点地乘上一个 $G</em>{\sigma_G^M}$，作为 loss function 里面的 $l_2$ loss 部分。因为 MS-SSIM 反向传播在 q 这个像素点的错误的时候是基于它对于中心点 $\tilde{q}$ 的 MS-SSIM 的贡献的，而这个贡献是由高斯权重来确定的。</p></li><li><p>MS-SSIM + $l_1$ Loss:<br>使用一个把 MS-SSIM loss 和 $l_1$ loss 加权合并起来的结果：</p><script type="math/tex; mode=display">L^{MS-SSIM-l_1} = \alpha \cdot L^{MS-SSIM} + (1 - \alpha) \cdot G_{\sigma_G^M} \cdot L^{l_!}</script><p>$l_1$ loss 也用相似的方法加权。</p></li></ul><p>我们使用了 1000 张图片（室内室外都有）作为验证集，剩下的用于训练。最开始的学习率是 0.01，mini-batch 大小是 8，所有的权重用高斯随机变量初始化（除非特殊指定）。我们设定动量是 0.9，weight decay 是 0.0001。我们也进行 gradient clipping，阈值是 $\pm 0.1$。所有的模型在 GTX 1070 上训练 14 个 epoch,经验上来看会收敛。对于 SSIM loss，$\sigma_G$ 设置为 5，$C_1$ 和 $C_2$ 是 0.01 和 0.03。</p><p>实验表明单单是替换 loss function 就会得到显著的性能上变化。其中 $MS-SSIM-l_2$的效果最好。然后我们进一步对这个模型 fine-tune，（使用一个预先训练好的 AOD-Net 作为 warm initialization，并使用更小的学习率和更大的 mini-batch 大小）。</p><h3 id="辅助目标检测的去雾"><a href="#辅助目标检测的去雾" class="headerlink" title="辅助目标检测的去雾"></a>辅助目标检测的去雾</h3><h4 id="使用-detection-module-串联来增强去雾效果"><a href="#使用-detection-module-串联来增强去雾效果" class="headerlink" title="使用 detection module 串联来增强去雾效果"></a>使用 detection module 串联来增强去雾效果</h4><p>作者表示，看到之前 AOD-Net + Faster-RCNN 的效果不错，决定尝试使用不同模型串联：去雾模型选择 DCP，DehazeNet，AOD-Net，DCPDN；目标检测模型选择 Faster R-CNN，SSD，RetinaNet，Mast-RCNN。然后由于有雾的图像常常对比度较低，我们还使用了增强对比度的模型（CLAHE）。</p><p>实验结果表明，直接简单地把目标检测模型换成更复杂的模块并不能显著提升去雾检测的心梗，因为有雾/去雾后图片和干净的图片（典型检测模块在这上面训练的）是有 domain gap 的。更加复杂的检测模型可能会在干净图片这个 domain 上面 overfit 了，有一次证明了在解决真实世界检测问题时处理 domain shift 的重要性。此外，一个更好地去雾模型(从复原效果上来看）并没有在它预先处理过的图片上更好的检测效果（比如 DPDCN）。而且，添加去雾的与处理过程后并不总能保证得到更好的检测效果。我们发现实验中 AOD-Net 倾向于生成平滑但是对比度较低的结果，有可能对 detection 造成影响。这样考虑的话，我们设计两个三阶段的串联模型，发现用 DCP 处理 AOD-Net 的去雾结果之后可以得到更好地结果（而且对比度增强）。</p><h4 id="Domain-Adaptive-Mask-RCNN"><a href="#Domain-Adaptive-Mask-RCNN" class="headerlink" title="Domain-Adaptive Mask-RCNN"></a>Domain-Adaptive Mask-RCNN</h4><p>我们受 domain adaptive Faster-RCNN 的启发，设计了 Domain-Adaptive Mask-RCNN。</p><p>DMask-RCNN 的首要目标是对 feature extraction 网络生成的特征进行 mask 操作，使得它具有 domain invariant 的性质（干净的图片和有雾的图片两个 domain）。具体来说，它在 Mask-RCNN 的基础的特征提取卷积神经网络后放了一个 domain adaptive 结构，引入一个 domain classfier 的 cross entropy loss。</p><p>文中实验结论：</p><ol><li>Domain-Adaptive 的检测方法非常 promising，效果远好于之前的方法</li><li>使用合适的 domain adaptation 可以充分发挥 detection model 的能力。另外，在没有 joint tuning 的时候，MSNN  效果比 AOD-Net 好。</li><li>target domain 的选取对结果影响较大，应当选取 domain discrepancy 较小的。</li><li>实验结果表明 MSCNN + DMask-RCNN 的串联模型效果最好</li></ol><h3 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h3><p>typo 很多，竟然这样就可以发 arxiv = =</p><p>upd: 竟然是 course project….</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;感觉周末好像不在状态啊，一直在浪，昨天换了设备之后迁移一晚上，然后又开始浪= =&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improved Techniques for Learning to Dehaze and Beyond: A Collective Study&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
  </entry>
  
  <entry>
    <title>python对比两个图片的差异并记录</title>
    <link href="https://ir1d.cf/2018/07/15/python%E5%AF%B9%E6%AF%94%E4%B8%A4%E4%B8%AA%E5%9B%BE%E7%89%87%E7%9A%84%E5%B7%AE%E5%BC%82%E5%B9%B6%E8%AE%B0%E5%BD%95/"/>
    <id>https://ir1d.cf/2018/07/15/python对比两个图片的差异并记录/</id>
    <published>2018-07-14T16:48:40.000Z</published>
    <updated>2018-07-30T00:49:37.215Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image, ImageChops, ImageStat</span><br><span class="line">im1 = Image.open(&quot;a.jpg&quot;)</span><br><span class="line">im2 = Image.open(&quot;b.jpg&quot;)</span><br><span class="line">diff = ImageChops.difference(im1, im2)</span><br><span class="line">diff.show()</span><br><span class="line">diff.save(&quot;c.jpg&quot;)</span><br><span class="line">stat = ImageStat.Stat(diff)</span><br><span class="line">sum_channel_values = sum(stat.mean)</span><br><span class="line">max_all_channels = len(stat.mean) * 100</span><br><span class="line">diff_ratio = sum_channel_values/max_all_channels</span><br><span class="line">print(diff_ratio)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="python" scheme="https://ir1d.cf/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.13</title>
    <link href="https://ir1d.cf/2018/07/13/dailynotes-7/"/>
    <id>https://ir1d.cf/2018/07/13/dailynotes-7/</id>
    <published>2018-07-13T01:02:33.000Z</published>
    <updated>2018-07-14T16:48:05.982Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Deep residual learning for image recognition</li></ul><a id="more"></a><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><h3 id="Jaccard-index"><a href="#Jaccard-index" class="headerlink" title="Jaccard index"></a>Jaccard index</h3><p>用来衡量两个集合的相似程度。$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$</p><h2 id="Deep-residual-learning-for-image-recognition"><a href="#Deep-residual-learning-for-image-recognition" class="headerlink" title="Deep residual learning for image recognition"></a>Deep residual learning for image recognition</h2><p>为什么 ResNet 和 Inception 网络可以得到比 VGG 更高的精度和更小的网络，但是在计算 perceptual loss 的时候还是一直在使用 VGG 的 feature map 呢？</p><p>看到一种解释是 VGG 是逐层训练得到的，网络是分段卷积，是一个效果良好的 feature extractor。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Deep residual learning for image recognition&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.12</title>
    <link href="https://ir1d.cf/2018/07/12/dailynotes-6/"/>
    <id>https://ir1d.cf/2018/07/12/dailynotes-6/</id>
    <published>2018-07-12T01:56:22.000Z</published>
    <updated>2018-07-14T16:48:00.526Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Network In Network</li><li>Going deeper with convolutions</li></ul><a id="more"></a><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><h3 id="Weight-decay"><a href="#Weight-decay" class="headerlink" title="Weight decay"></a>Weight decay</h3><p>之前是$w = w - \eta * \frac{\partial L}{\partial W}$</p><p>加上 weight decay 之后就是<br>$w = w <em> 0.99 - \eta </em> \frac{\partial L}{\partial W}$</p><p>相当于是 regularization 操作，用于防止过拟合</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>$v = \beta <em> v - \alpha </em> dx$<br>$x = x + v$</p><p>其中 $\beta$ 是 Momentum 系数</p><h2 id="Network-In-Network"><a href="#Network-In-Network" class="headerlink" title="Network In Network"></a>Network In Network</h2><p>之前的卷积结构相当于是在对上一层特征做仿射变换，作者指出如果索要提取的特征不是线性可分的那么需要使用大量的滤波器。作者认为可以在卷积过程中选择对感受野进行更加复杂的操作，本文中介绍了 MLP 卷积层的设计。</p><p>MLP 卷积层的设计就是用多层 fc 代替。</p><script type="math/tex; mode=display">f_{i, j, k_1}^1 = max((w_{k_1}^1)^Tx_{i,j} + b_{k_1}, 0)</script><p>…</p><script type="math/tex; mode=display">f_{i, j, k_1}^n = max((w_{k_n}^n)^Tf_{i,j}^{n - 1} + b_{k_n}, 0)</script><p>这里的 n 表示 multilayer perceptron 是 n 层的。这个过程也可以看作是对每一位置的不同通道进行线性变换（跨通道的交互和信息整合），相当于是在做<code>1 x 1</code>的卷积。</p><p>对多个线性函数进行 maxout 操作可以得到一个分段函数，这样的分段函数可以近似任何一个凸函数。</p><script type="math/tex; mode=display">f_{i, j, k} = max_m{w^T_{km}x_{i, j}}</script><p>可以看出二者是不同的。</p><p>文中还提出了一个操作是叫全局均值池化。之前的网络结构是把最后一个卷积层的输出传给全连接层然后再给一个 softmax 层做 logistic regression 。这个结构把卷积层看作是特征提取工具。但是这里面的全连接层很容易发生过拟合，导致整个网络的泛化性能不好。</p><p>本文中的全局平均池化是为了代替这个全连接层。文中的方法是对每一种类别生成一个 feature map 之后，并没有再加一个全连接层上去，而是直接对这些 feature map 内部求平均（每张 map 平均成一个数），之后再用 softmax 处理一下。这样做的好处是得到的信息更接近卷积层得到的分类信息，feature map 就可以转化为 categories confidence map。另一个好处是池化的过程没有额外的参数，可以避免过拟合。这个过程实际上就是强制网络学到用 feature map 来表示分类结果的方法。</p><p>这里发现了之前的一个错误理解，<code>1 x 1</code>的卷积是多个 feature channel 的线性叠加，这里 channel 不一定只有一个，所以不一定是只能用来 scaling。事实上，可以起到降维、通过后面接的非线性函数来增强非线性性之类的效果。</p><h2 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h2><p>本文中指出 <code>1 x 1</code> 的卷积有两个用处，一是用来降维，二是用来去除计算瓶颈，这使得网络结构可以变深、变宽。</p><p>R-CNN 把目标检测的问题分成两个子问题：利用 low-level 的信息（比如颜色和纹理来生成目标区域），和利用 CNN 分类器对得到的区域里的物体进行分类。</p><p>文中表示神经网络努力变深变宽带来一个问题是参数变多，另一个问题是会导致在 filter 数量上升的时候，深层网络的计算成本是指数级增长的。文中提出了解决办法，可以引入 sparsity。但是稀疏的数据会在计算过程中产生额外的成本。</p><p>文中提出了 Inception 结构，主要是考虑如何用现有的组件来近似卷积网络，同时降低成本。文中引入<code>1 x 1</code>的卷积来实现降维，把降维后的信息在传给代价昂贵的<code>3 x 3</code>和<code>5 x 5</code>。多引入的卷积层还附带了非线性激活函数，增强模型的非线性性。最后还使用了 max-pooling 层来把网络的大小减半。作者表示，实验发现目前结构仍然不是很理想，适合在更高的层使用而让低层结构使用传统的卷积神经网络结构。</p><p>这个结构的优点之一是使得每一阶段增加节点数不会导致之后阶段的总计算复杂度 “uncontrolled blow-up”。作者认为这一设计和传统直觉是一致的，处理图像信息的时候从不同尺度来捕捉。</p><p>网络中在一层里包含多个卷积核，这样每一层都能学到稀疏和不稀疏的特征。另外文中引入了 DepthConcat 来对的一层里的信息整合，代替了常见的 ReLU 之类的非线性激活函数。</p><p>ref: <a href="https://stats.stackexchange.com/questions/184823/how-does-the-depthconcat-operation-in-going-deeper-with-convolutions-work" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/184823/how-does-the-depthconcat-operation-in-going-deeper-with-convolutions-work</a></p><p>然后作者表示之前 Alexnet 使用的 cropping approach 看起来很不错，我们也用了，虽然优点不是很大。就是测试的时候把图片按照（256\，288，329，352 四种大小）切出来的四个角落和中间部分喂给网络输入。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Network In Network&lt;/li&gt;
&lt;li&gt;Going deeper with convolutions&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.11</title>
    <link href="https://ir1d.cf/2018/07/11/dailynotes-5/"/>
    <id>https://ir1d.cf/2018/07/11/dailynotes-5/</id>
    <published>2018-07-11T00:29:31.000Z</published>
    <updated>2018-07-14T16:47:53.322Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Very deep convolutional networks for large-scale image recognition</li></ul><a id="more"></a><h2 id="Very-deep-convolutional-networks-for-large-scale-image-recognition-VGG"><a href="#Very-deep-convolutional-networks-for-large-scale-image-recognition-VGG" class="headerlink" title="Very deep convolutional networks for large-scale image recognition (VGG)"></a>Very deep convolutional networks for large-scale image recognition (VGG)</h2><p>本文中对数据的预处理也用了 Alexnet 里提到的方法，就是把数据的像素 RGB 值减去整个训练集的平均值。同时还利用了<code>1 x 1</code>的卷积核，文中认为<code>1 x 1</code>的卷积核相当于是对输入的通道做了一次线性变换。文中还提到不是所有的卷积层后面都跟着 max-pooling 层，感觉也是很神奇。</p><p>然后作者表示 Alexnet 的 LRN 操作并没有起到多大作用。<br>作者认为在整个网络里使用<code>3 x 3</code>的卷积核可以让网络获得深度（其中会有更多的 rectification 层来让判别），另一方面通过堆叠（三层）<code>3 x 3</code>来获得<code>7 x 7</code>的感受域的话所需权重的数量要少一些，减少训练过程中的内存占用。文中认为使用<code>1 x 1</code>的卷积（后面跟着 ReLU ）可以增强决策函数的非线性性而不对感受域的大小造成影响。本质上可以把<code>1 x 1</code>的卷积看作是在空间中的一次线性变换。</p><p>VGG 里面也使用了 dropout。作者表示我们的方案比 Alexnet 需要训练的 epoch 数要少，一是因为更深的网络和更小的卷积核隐式地起到了 regularize 的功能，二是因为一些层进行了参数的初始化。</p><p>在训练的时候先预训练一个图片大小固定的（384，训 384 的时候用训 256 的时候的参数初始化），然后再训练的时候用 384 的参数来初始化，每处理一张图的时候，都从$[256, 512]$里随机一个数来表示图片的大小。作者认为这样的操作可以捕捉由于图像中目标大小不一致引入的信息。</p><p>文中表示做了数据的水平翻转来增强，但是没有用 Alexnet 里面提到的那个多次取样放到网络里训，因为这里的网络结构用卷积层替换掉了原来的全连接层，就可以直接在整张图上操作了。</p><p>实验部分中，作者提到选用多个模型融合（对结果求平均）可以取得更好地结果。</p><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><blockquote><p>Stochastic Gradient Descent</p></blockquote><p>考虑到最初的梯度下降对于每次迭代都使用全部的样本，成本较高，SGD 简化了这个过程，在每次迭代的时候只对一个样本计算梯度，可以看做是 online learning 。</p><blockquote><p>Mini-Batch Gradient Descent</p></blockquote><p>SGD 每次只用一个样本，如果遇上了噪声则很容易陷入局部最优解，所以使用 mini-batch 的思想，每次使用一批放进去计算梯度来更新。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Very deep convolutional networks for large-scale image recognition&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.10</title>
    <link href="https://ir1d.cf/2018/07/10/dailynotes-4/"/>
    <id>https://ir1d.cf/2018/07/10/dailynotes-4/</id>
    <published>2018-07-09T16:43:38.000Z</published>
    <updated>2018-07-11T00:53:05.450Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Imagenet classification with deep convolutional neural networks</li></ul><a id="more"></a><h2 id="Imagenet-classification-with-deep-convolutional-neural-networks"><a href="#Imagenet-classification-with-deep-convolutional-neural-networks" class="headerlink" title="Imagenet classification with deep convolutional neural networks"></a>Imagenet classification with deep convolutional neural networks</h2><p>本文提出的 AlexNet，突破性地在 ImageNet 数据集上取得了极好的效果。作者特别自信地表示如果更快的 GPU 和更大的数据集出现，他们可以得到更好地结果。</p><p>我注意到一点很有趣的事情，他们处理数据的时候是直接把短边变成 256，然后直接裁中间的那部分。感觉这样很有可能导致最后的输入图像里只有部分信息，不知道是不是都这么做的。（感觉也没有别的办法了？总不能先跑一遍 detection？）另外，他们把图像的像素减去了数据集像素的平均值。</p><p>网络总共有8层，其中有5个卷积层和3个全连接层。</p><p>文中提出使用 ReLU 是不饱和的非线性函数，在梯度下降的时候训练速度要快。像$f(x)=tanh(x)$ 和 $f(x)=(1+e^{-x})^{-1}$是饱和的非线性函数（从图像可以看出）。</p><p>由于网络结构太大显存放不下，本文作者在两个 GPU 上面并行地跑。其中本文要求 GPU 只在特定的层才相互交换信息，来控制信息交换的代价在整个计算过程中的占比。</p><p>Local Response Normalization，翻译过来是局部响应归一化。首先又夸了 ReLU 的好处是不需要对输入数据做归一化因为它不饱和 —— 只要给 ReLU 的输入是正的，它的梯度就不为0。然后作者表示发现 local normalization 操作一下之后可以让网络有更好的泛化效果。注意$a_{x,y}^{i}$表示的是在$(x, y)$这个位置操作第i个卷积核的结果。局部归一化就是对每一个a，用相邻的n个卷积核结果来归一化。</p><script type="math/tex; mode=display">b_{x, y}^{i} = a_{x, y}^{i} / {\left( k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x, y}^{j})^2 \right)}^ \beta</script><p>其中 $k,n,\alpha, \beta$ 是超参数，在 validation set 上面决定的。这里作者并没有给数据减去它们的平均值。这个操作放在一部分 ReLU 层之后。好像说是 LRN 效果并不明显，在日后新网络结构中使用很少</p><p>作者表示发现如果允许池化层池化的部分又重叠，可以让网络不那么容易就过拟合、并减少错误率。</p><p>网络目标是最大化一个多项式的 logistic 回归，相当于是最大化数据输出的正确类别的可能性的平均值</p><p>文中使用的数据增强方法只需要很少的计算，这样增强之后多出来的数据不需要提前放到硬盘上。实现中用 CPU 来增强数据， GPU 同时训练上一批数据，所以对计算资源没有影响。第一种增强的方法是从<code>256 x 256</code>的图片中随机的取<code>224 x 224</code>的块，然后也加入水平翻转后的结果，这样就把数据集扩大为原来的2048倍。测试的时候也是取出来四个角落和中心的<code>224 x 224</code>的块以及他们的水平翻转，把这十张图的结果求平均当做是原图的测试结果。</p><p>第二种数据增强的方法是改变输入数据 RGB 通道亮度。文中对每张图片都做 PCA，$p_1, p_2, p_3$是特征向量，$\lambda_1, \lambda_2, \lambda_3$是特征值。每次图像被用来训练的时候，都重新生成三个$\alpha_i$，然后把原图上的每个像素加上$[p_1, p_2, p_3][\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T$。这里面的$\alpha_i$是从一个均值为0，标准差为0.1的高斯随机数发生器产生，注意这里对于同一张图在同一次训练过程中使用同一组$\alpha_i$。作者表示这样操作可以使得学到的物体特征不受亮度和光照颜色的影响，captures an important property of natural images。文中表示实验结果证明可以减少错误率。</p><p>同时还使用了 dropout，就是让前两个全连接层的神经元有一定概率（文中0.5）不参与前向和反向传播。这样每次数据输入进来的时候，神经网络都给出一个不一样的结构，但是这些结构是共享权重的。这种操作可以减少神经元之间的依赖，因为如此下来神经元不能依赖于某个其他神经元的存在来处理信息，也就迫使神经元尝试学习到更加强大的可以和其他神经元的随机子集共同工作的特征。在测试的时候，本文中使用全部的节点，只不过把它们的输出都乘上0.5</p><p>作者表示初始化权重的时候用标准差为0.01的零均值高斯分布。给某些卷积层的神经元偏置设为1，使得初期阶段传给 ReLU 的输入是正的，其余层的偏置用0初始化。另外，如果在验证集上的错误率停止进步，就把学习率除以10</p><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><blockquote><p>Toeplitz matrix</p></blockquote><p>a.k.a diagonal-constant matrix.</p><p>主对角线上的元素相等，$A<em>{i,j} = A</em>{i+1,j+1}$</p><p>不一定是个方阵</p><blockquote><p>Factorial Distribution</p></blockquote><p>要求一组变量中变量之间没有相互作用</p><p>$P(x|y) = P(x)$</p><blockquote><p>non-saturating neurons</p></blockquote><p>不饱和神经元，就是说梯度不接近0，还可以有效地调整权重</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Imagenet classification with deep convolutional neural networks&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu防止打字误触触摸板</title>
    <link href="https://ir1d.cf/2018/07/09/ubuntu%E9%98%B2%E6%AD%A2%E6%89%93%E5%AD%97%E8%AF%AF%E8%A7%A6%E8%A7%A6%E6%91%B8%E6%9D%BF/"/>
    <id>https://ir1d.cf/2018/07/09/ubuntu防止打字误触触摸板/</id>
    <published>2018-07-09T06:05:33.000Z</published>
    <updated>2018-07-09T06:06:36.009Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:atareao/atareao</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install touchpad-indicator</span><br></pre></td></tr></table></figure><p>ref: <a href="http://ubuntuhandbook.org/index.php/2018/04/install-touchpad-indicator-ubuntu-18-04-lts/" target="_blank" rel="noopener">http://ubuntuhandbook.org/index.php/2018/04/install-touchpad-indicator-ubuntu-18-04-lts/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=
      
    
    </summary>
    
      <category term="操作" scheme="https://ir1d.cf/categories/%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="ubuntu" scheme="https://ir1d.cf/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.9</title>
    <link href="https://ir1d.cf/2018/07/09/dailynotes-3/"/>
    <id>https://ir1d.cf/2018/07/09/dailynotes-3/</id>
    <published>2018-07-09T00:07:32.000Z</published>
    <updated>2018-07-10T12:39:56.525Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Image De-raining Using a Conditional Generative Adversarial Network</li><li>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</li><li>Image-to-Image Translation with Conditional Adversarial Networks</li></ul><a id="more"></a><h2 id="Image-De-raining-Using-a-Conditional-Generative-Adversarial-Network"><a href="#Image-De-raining-Using-a-Conditional-Generative-Adversarial-Network" class="headerlink" title="Image De-raining Using a Conditional Generative Adversarial Network"></a>Image De-raining Using a Conditional Generative Adversarial Network</h2><p>本文提出了用conditional-GAN来生成无雨背景的方法</p><p><img src="https://i.loli.net/2018/07/09/5b42bd0d27da8.png" alt="ID-GAN的网络结构"></p><p>生成器从雨图中学习生成无雨背景，然后判别器来判断是真实背景还是生成的</p><p>然后提出损失函数中不应只有 perceptual loss，还应当加入 per-pixel 的 loss 和 adversarial loss 一起，不过文中并没有提到三个loss的权重是怎么学出来的</p><p>最后提到缺点是不能处理图像上的白块，猜测可能是数据集中没有出现类似情况导致的，以及指出数据集难以包含整个 rain streak 的可能情况</p><h2 id="Unsupervised-Representation-Learning-with-Deep-Convolutional-Generative-Adversarial-Networks"><a href="#Unsupervised-Representation-Learning-with-Deep-Convolutional-Generative-Adversarial-Networks" class="headerlink" title="Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"></a>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</h2><p>本文介绍了从大量无标签数据集中学习可重复使用的特征表示的一些结果</p><p>文中也展示在图像数据集上用无监督的方法也可以得到很有趣的层次信息</p><p>文中总结了优化GAN的一些方法：</p><ul><li>在判别器用 strided convolution, 在生成器用 fractional convolution 来代替所有的 pooling 层</li><li>在判别器和生成器中用 batch normalization</li><li>用更深的结构来替换掉全连接的隐藏层</li><li>生成器里除了最后一层都用 ReLU 来做 activation，最后一层用 Tanh</li><li>判别器里所有层都用 LeakyReLU 做 activation （激活函数）</li></ul><h2 id="Image-to-Image-Translation-with-Conditional-Adversarial-Networks"><a href="#Image-to-Image-Translation-with-Conditional-Adversarial-Networks" class="headerlink" title="Image-to-Image Translation with Conditional Adversarial Networks"></a>Image-to-Image Translation with Conditional Adversarial Networks</h2><p>很多视觉的 task 可以看作是在把输入图像翻译成输出图像，本文提出的 cGAN 技术可以学到这种翻译的对应关系，也学到一个对应的 loss function 来训练这个对应关系。最为著名的应用是 pix2pix。</p><p>conditional GAN 学习一个 structured loss，惩罚网络的输出在结构上的不同。</p><p>这里的 conditional 的最大区别在于，输入的 edge map 也要喂给判别器。同时文中提出，使用 L1 距离代替 L2 距离来要求生成器的输出和 groud truth 更近，可以鼓励减少模糊。</p><p><code>U-net</code>看起来就是加上了 skip connection 的 encoder-decoder network 。比如给图片上色的时候，突出的边缘的位置信息是输入和输出共享的，如果直接把它们从输入层传到输出层（避免经过 bottle neck），情况会好很多。具体来说，<code>U-net</code>形式里面把第 i 层和第 n-i 层里所有的通道都通过 skip connection 直接连接起来</p><p>PatchGAN，就是每次对结构的惩罚是在一个 patch 的层级来操作的。就是对整个图而言，是每次考虑一个 <code>n * n</code> 的 patch 是真的还是假的。实验证明，甚至 n 和整个图的大小相比可以很小。这样下来判别器相当于把图片看作是一个 Markov random field，其中假设了距离超过一个 patch 的直径的像素点之间是独立的。这样的 PatchGAN 可以看作是一种 材质/风格 的 loss</p><p>文中还提到了一些优化，比如生成器要最大化$log\ D(x, G(x, z))$而不是最小化$log\ (1 - D(x, G(x, z)))$；另外在训练判别器的时候，把目标函数除以 2，用来减小 D 相对于 G 学习的速度，然后每次 D 下降一步，G 也下降一步。</p><p>文中还有个很神奇的地方，作者表示在 inference time，生成网络和训练的时候是完全一样使用的，就是说这里面的 dropout 部分在测试的时候也没有移除。</p><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><blockquote><p>image patch</p></blockquote><p>图像块的意思= =</p><blockquote><p>Leaky ReLUs</p></blockquote><p>leaky ReLU 的特点是在 $x \leq 0$ （inactive）的时候允许有一个小的正梯度（0.01）</p><p>而 Parametric ReLU 在这里更进一步，让那个正梯度变成一个参数，在神经网络反向传播的时候可以得到更新</p><p>注意在$a \leq 1$的时候，PReLU 相当于 $max(x, ax)$，这个形式和 maxout 网络有一定关联</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Image De-raining Using a Conditional Generative Adversarial Network&lt;/li&gt;
&lt;li&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/li&gt;
&lt;li&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
      <category term="GAN" scheme="https://ir1d.cf/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.8</title>
    <link href="https://ir1d.cf/2018/07/08/dailynotes-2/"/>
    <id>https://ir1d.cf/2018/07/08/dailynotes-2/</id>
    <published>2018-07-07T23:40:44.000Z</published>
    <updated>2018-07-10T12:47:53.326Z</updated>
    
    <content type="html"><![CDATA[<p>什么也没读，沉迷标数据不能自拔</p><ul><li>Joint Bi-layer Optimization for Single-image Rain Streak Removal</li></ul><a id="more"></a><p>作息过于糟糕，下午回去看数分结果很快就睡着了，还是闹钟免疫的那种。。。。</p><h2 id="Joint-Bi-layer-Optimization-for-Single-image-Rain-Streak-Removal"><a href="#Joint-Bi-layer-Optimization-for-Single-image-Rain-Streak-Removal" class="headerlink" title="Joint Bi-layer Optimization for Single-image Rain Streak Removal"></a>Joint Bi-layer Optimization for Single-image Rain Streak Removal</h2><p>本文做的是去除单张照片上雨纹的工作，具体来说是尝试把一张照片分成有雨和无雨的两层。整个最优化过程是同时把雨纹的细节从无雨层去掉，和把无雨的信息从雨纹层去掉.</p><p>拿到一张图，文中方法首先尝试确定 rain-dominated 的区域，然后利用这部分区域来确定雨的方向，并提取出 rain-patch，之后用 joint bi-layer optimization 方法来迭代地得到两层。</p><p>我怎么有个问题。。感觉一张图里的 rain streak 完全有可能角度差得很大啊？ 比如前景和后景完全可以反向</p><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><blockquote><p>Frobenius norm</p></blockquote><p>a.k.a Euclidean norm, $\left\lVert A \right\rVert = \sqrt{\sum<em>{i=1}^m{\sum</em>{j=1}^n{|a_{ij}|^2}}}$</p><blockquote><p>fidelity term</p></blockquote><p>保真项</p><blockquote><p>Jensen’s inequality</p></blockquote><p>The mean value of a convex function is never lower than the value of the convex function applied to the mean. </p><script type="math/tex; mode=display">\begin{align}- \log p(x) &= - \log \int p(x,y) dy \\&= - \log \int q(y\vert x) \frac{p(y,x)}{q(y\vert x)}dy \\&\leq - \int q(y\vert x) \log \frac{p(y,x)}{q(y\vert x)} dy\end{align}</script><blockquote><p>L0 regularization</p></blockquote><p>l0 norm 是统计数组中的非零元素</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;什么也没读，沉迷标数据不能自拔&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Joint Bi-layer Optimization for Single-image Rain Streak Removal&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
      <category term="paperReading" scheme="https://ir1d.cf/tags/paperReading/"/>
    
  </entry>
  
  <entry>
    <title>Daily Notes 7.7</title>
    <link href="https://ir1d.cf/2018/07/07/dailynotes-1/"/>
    <id>https://ir1d.cf/2018/07/07/dailynotes-1/</id>
    <published>2018-07-06T16:41:13.000Z</published>
    <updated>2018-07-10T12:43:41.579Z</updated>
    
    <content type="html"><![CDATA[<p>学数学 + 下午睡过头了</p><a id="more"></a><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><blockquote><p> Checkerboard Artifacts </p></blockquote><p>Checkerboard Pattern 是黑白格交替的图案</p><p>神经网络生成的图像在细节处往往会出现像素点颜色交替的现象</p><p>ref: <a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">https://distill.pub/2016/deconv-checkerboard/</a></p><p>transposed-conv 时出现 overlap 会导致这种现象</p><blockquote><p>Sobel operator</p></blockquote><p>用于 edge detection</p><p><a href="https://en.wikipedia.org/wiki/Sobel_operator" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Sobel_operator</a></p><blockquote><p>skip connection</p></blockquote><p>用在 resnet 里面</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学数学 + 下午睡过头了&lt;/p&gt;
    
    </summary>
    
      <category term="summer2018" scheme="https://ir1d.cf/categories/summer2018/"/>
    
    
  </entry>
  
</feed>
